{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Netflix TV Shows and Movies Dataset Analysis\n",
    "\n",
    "This notebook presents an exploratory data analysis (EDA) of the Netflix dataset using Scala and Apache Spark. \n",
    "The dataset contains information about TV shows and movies available on Netflix, including their release dates, \n",
    "genres, and ratings. The analysis focuses on identifying trends, understanding the distribution of content, \n",
    "and deriving meaningful insights.\n",
    "\n",
    "## Steps Covered:\n",
    "1. **Data Cleaning and Preprocessing**\n",
    "2. **Exploratory Data Analysis (EDA)** using Spark SQL and Scala functions\n",
    "3. **Visualization of Trends** such as content release years, genres, and ratings distribution\n",
    "\n",
    "**Tools Used**:\n",
    "- **Apache Spark** for distributed data processing\n",
    "- **Scala** for defining transformations and performing analytics\n",
    "- **Plotly** for generating visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99072113, 3)\n",
      "   user_id movie_id rating\n",
      "0  1488844        1      3\n",
      "1   822109        1      5\n",
      "2   885013        1      4\n",
      "3   823519        1      3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val netflixDF = spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"netflix_titles.csv\")\n",
    "\n",
    "netflixDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice, this below 4 cells are for first part only(1/4)\n",
    "To understand the data a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of movies in part_1 : 4499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Check if the Rating column is null, if so it means it is 'movie id' row, count them \n",
    "movie_count = df1.isnull().sum()[1]\n",
    "print('number of movies in part_1 : {}'.format(movie_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Users in part_1 : 470758\n"
     ]
    }
   ],
   "source": [
    "# Count total number of rows and subtract movies' count \n",
    "print('number of Users in part_1 : {}'.format(df1['user_id'].nunique()- movie_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations in part_1 : 24053764\n"
     ]
    }
   ],
   "source": [
    "# Count total number of observations \n",
    "num_observations = df1['user_id'].count()- movie_count\n",
    "print('number of observations in part_1 : {}'.format(num_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          user_id\n",
      "rating           \n",
      "1.0      4.648694\n",
      "2.0     10.140089\n",
      "3.0     28.703121\n",
      "4.0     33.615284\n",
      "5.0     22.892812\n"
     ]
    }
   ],
   "source": [
    "# Trying to see the uniformity in the data \n",
    "print(df1.groupby('rating').count()* 100/num_observations)\n",
    "\n",
    "\n",
    "# 85% of the data has  3 or more than rating\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to remove ID from rows and put into a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4403, 1)\n",
      "True\n",
      "26847523\n"
     ]
    }
   ],
   "source": [
    "### Use below code to process each part one by one, also keep updating the movie_id variable\n",
    "\n",
    "df_nan = pd.DataFrame(pd.isnull(df1.rating))\n",
    "df_nan = df_nan[df_nan['rating']==True]\n",
    "print(df_nan.shape)\n",
    "\n",
    "# create a single array with movie id - size ( difference of index) and value ( 1,2,3 etc)\n",
    "\n",
    "movie_np = []\n",
    "\n",
    "## We keep changing this variable by manually looking up the movie_id in one of those 4 data files\n",
    "## As of now, this is to process the 4th part\n",
    "movie_id = 13368\n",
    "\n",
    "for i,j in zip(df_nan.index[1:],df_nan.index[:-1]):\n",
    "#     print(i,j)\n",
    "    temp_arr = np.full((1,i-j-1), movie_id)\n",
    "    movie_np = np.append(movie_np,temp_arr)\n",
    "    movie_id += 1\n",
    "\n",
    "# last movie id\n",
    "\n",
    "print(df_nan.iloc[-1, 0])\n",
    "\n",
    "r = np.full((1,len(df1) - df_nan.index[-1] -1), movie_id)\n",
    "#print(temp_arr)\n",
    "movie_np = np.append(movie_np,r)\n",
    "print(len(movie_np))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'rating', 'movie_id'], dtype='object')\n",
      "          user_id  rating  movie_id\n",
      "1         2385003     4.0     13368\n",
      "5000907    495207     1.0     14274\n",
      "10001561  2243515     4.0     14928\n",
      "15002391  2210687     5.0     15758\n",
      "20003238  2088300     5.0     16605\n",
      "25004038   717559     3.0     17405\n",
      "persist the processed file \n"
     ]
    }
   ],
   "source": [
    "### Append the movie_np array as a column to the dataframe\n",
    "###\n",
    "df1 = df1[pd.notnull(df1['rating'])]\n",
    "#Add the movie_id column\n",
    "df1['movie_id'] = movie_np.astype(int)\n",
    "df1['user_id'] = df1['user_id'].astype(int)\n",
    "print(df1.columns)\n",
    "\n",
    "print(df1.iloc[::5000000,:])\n",
    "\n",
    "new_cols = df1.columns.tolist()\n",
    "new_cols = new_cols[:1]+new_cols[-1:]+new_cols[1:2]\n",
    "df1 = df1[new_cols]\n",
    "\n",
    "print(\"persist the processed file \")\n",
    "df1.to_csv(\"processed_part4.txt\", encoding='utf-8', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4499, 470758)\n"
     ]
    }
   ],
   "source": [
    "## Here we bring data into the forma of a matrix\n",
    "## BUT, we don't use it\n",
    "df_p = pd.pivot_table(df1,values='rating', index='movie_id', columns='user_id')\n",
    "print(df_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below is just to process the probe file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id  movie_id\n",
      "0          30878         1\n",
      "1        2647871         1\n",
      "2        1283744         1\n",
      "3        2488120         1\n",
      "4         317050         1\n",
      "5        1904905         1\n",
      "6        1989766         1\n",
      "7          14756         1\n",
      "8        1027056         1\n",
      "9        1149588         1\n",
      "10       1394012         1\n",
      "11       1406595         1\n",
      "12       2529547         1\n",
      "13       1682104         1\n",
      "14       2625019         1\n",
      "15       2603381         1\n",
      "16       1774623         1\n",
      "17        470861         1\n",
      "18        712610         1\n",
      "19       1772839         1\n",
      "20       1059319         1\n",
      "21       2380848         1\n",
      "22        548064         1\n",
      "23       1952305        10\n",
      "24       1531863        10\n",
      "25       2326571      1000\n",
      "26        977808      1000\n",
      "27       1010534      1000\n",
      "28       1861759      1000\n",
      "29         79755      1000\n",
      "...          ...       ...\n",
      "1408366   194929      9995\n",
      "1408367  1847661      9995\n",
      "1408368    66828      9996\n",
      "1408369  1149582      9996\n",
      "1408370   336696      9996\n",
      "1408371  2462908      9996\n",
      "1408372  1589627      9996\n",
      "1408373  1720226      9996\n",
      "1408374  1194354      9996\n",
      "1408375   592532      9996\n",
      "1408376  1351081      9996\n",
      "1408377    80354      9996\n",
      "1408378    16792      9996\n",
      "1408379   481320      9996\n",
      "1408380   899431      9996\n",
      "1408381   570792      9996\n",
      "1408382  1619158      9996\n",
      "1408383  2571420      9996\n",
      "1408384  1817485      9996\n",
      "1408385  1206224      9996\n",
      "1408386  1553993      9996\n",
      "1408387  2179700      9997\n",
      "1408388  1347835      9997\n",
      "1408389   765578      9997\n",
      "1408390  2328701      9997\n",
      "1408391  1288730      9998\n",
      "1408392  2536567      9998\n",
      "1408393  1107317      9998\n",
      "1408394  1473765      9999\n",
      "1408395               9999\n",
      "\n",
      "[1408396 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Code to pre-process probe.txt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "probe_file = open(\"probe.txt\",\"r\")\n",
    "lines = probe_file.read().split(\"\\n\")\n",
    "\n",
    "movie_id_dict = {}\n",
    "prev_index = 0\n",
    "prev_value = 1\n",
    "\n",
    "valueList = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    if(\":\" in lines[i]):\n",
    "        movie_id_dict[str(prev_value)] = i-prev_index-1\n",
    "        prev_index = i\n",
    "        prev_value = int(lines[i].split(\":\")[0])\n",
    "    else:\n",
    "        valueList.append(lines[i])\n",
    "    \n",
    "movie_id_dict[str(prev_value)] = len(lines)-prev_index-1\n",
    "\n",
    "orderedDict = OrderedDict(sorted(movie_id_dict.items()))\n",
    "\n",
    "movie_np = []\n",
    "## Create np array based on the ordered dict \n",
    "\n",
    "for k,v in orderedDict.items():\n",
    "    temp_arr = np.full((1,v), int(k))\n",
    "    movie_np = np.append(movie_np,temp_arr)\n",
    "\n",
    "        \n",
    "df = pd.DataFrame(valueList,columns=['user_id'])\n",
    "df['movie_id'] = movie_np.astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below reads each processed part (remember there are 4) and processed_probe.txt \n",
    "After that, we create indices on (user_id,movie_id) and use set difference to filter out train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating\n",
      "0  2385003     13368     4.0\n",
      "1   659432     13368     3.0\n",
      "2   751812     13368     2.0\n",
      "3  2625420     13368     2.0\n",
      "4  1650301     13368     1.0\n",
      "5  2269227     13368     4.0\n",
      "6  2220672     13368     4.0\n",
      "7  2500511     13368     4.0\n",
      "8  1452058     13368     2.0\n",
      "9  1624891     13368     3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val netflixDF = spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"netflix_titles.csv\")\n",
    "\n",
    "netflixDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating\n",
      "0    30878         1     NaN\n",
      "1  2647871         1     NaN\n",
      "2  1283744         1     NaN\n",
      "3  2488120         1     NaN\n",
      "4   317050         1     NaN\n",
      "5  1904905         1     NaN\n",
      "6  1989766         1     NaN\n",
      "7    14756         1     NaN\n",
      "8  1027056         1     NaN\n",
      "9  1149588         1     NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val netflixDF = spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"netflix_titles.csv\")\n",
    "\n",
    "netflixDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train df\n",
      "26847523\n",
      "\n",
      "length of probe df\n",
      "1408395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"length of train df\")\n",
    "print(len(df_train))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"length of probe df\")\n",
    "print(len(df_probe))\n",
    "\n",
    "df_probe['movie_id'] = df_probe['movie_id'].astype(int)\n",
    "\n",
    "# for i in range(len(df_probe)):\n",
    "#     if (df_probe.iloc[i]['movie_id'] > 4499):\n",
    "#         df_probe.drop(df_probe.index[i], inplace=True)\n",
    "\n",
    "# print(len(df_probe))\n",
    "keys = ['user_id', 'movie_id']\n",
    "i1 = df_train.set_index(keys).index\n",
    "i2 = df_probe.set_index(keys).index\n",
    "df_pure_train =  df_train[~i1.isin(i2)]\n",
    "df_pure_train.to_csv(\"processed_pure_train_4.txt\", encoding='utf-8', index=False)\n",
    "\n",
    "## Now get probe\n",
    "i3 = df_pure_train.set_index(keys).index\n",
    "df_pure_probe = df_train[~i1.isin(i3)]\n",
    "\n",
    "\n",
    "df_pure_probe.to_csv(\"processed_pure_probe_4.txt\", encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the ALS based model code, using Apache Spark\n",
    "Please note, we are using the complete dataset here, all ~100 million records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training at :\n",
      "2017-11-20 17:42:49.902628\n",
      "\n",
      "Finished training at :\n",
      "2017-11-20 18:01:13.524056\n",
      "\n",
      "With Parameters\n",
      "lambda = 0.09\n",
      "Number of features = 15\n",
      "Mean Squared Error = 0.8727117236047375\n",
      "Root Mean Squared Error = 0.9341904107861189\n",
      "Finished All at :\n",
      "2017-11-20 18:02:10.742918\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "conf.set(\"spark.driver.memory\",\"4g\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"all_4_train.txt\")\n",
    "ratings = data.map(lambda l: l.split(','))\\\n",
    "    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "#print(ratings.collect()[0:5])\n",
    " \n",
    "print(\"Starting the training at :\")\n",
    "print(str(datetime.now()))\n",
    "print()\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 15\n",
    "numIterations = 10\n",
    "lambda_ = 0.09 # default is 0.01\n",
    "model = ALS.train(ratings, rank, numIterations, lambda_)\n",
    "\n",
    "print(\"Finished training at :\")\n",
    "print(str(datetime.now()))\n",
    "\n",
    "(\"Reading test or Probe file\")\n",
    "probe_data = sc.textFile(\"all_4_probe.txt\")\n",
    "\n",
    "ratings_probe = probe_data.map(lambda l: l.split(','))\\\n",
    "    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "# Evaluate the model on training data\n",
    "testdata = ratings_probe.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings_probe.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "RMSE = MSE**(0.5)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"With Parameters\")\n",
    "print(\"lambda = \"+ str(lambda_))\n",
    "print(\"Number of features = \"+str(rank))\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "print(\"Root Mean Squared Error = \" + str(RMSE))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Finished All at :\")\n",
    "print(str(datetime.now()))\n",
    "\n",
    "\n",
    "# # Save and load model\n",
    "# model.save(sc, \"CollabALS\")\n",
    "# sameModel = MatrixFactorizationModel.load(sc, \"CollabALS\")\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "Ehti",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "title": "Netflix TV Shows and Movies Analysis Using Scala and Apache Spark"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
